{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   make  address   all   3d   our  over  remove  internet  order  mail  ...  \\\n",
      "0  0.00     0.00  0.29  0.0  0.00  0.00    0.00      0.00   0.00  0.00  ...   \n",
      "1  0.46     0.00  0.00  0.0  0.00  0.00    0.00      0.00   0.00  0.00  ...   \n",
      "2  0.00     0.00  0.00  0.0  0.00  0.00    0.00      0.00   0.00  0.00  ...   \n",
      "3  0.33     0.44  0.37  0.0  0.14  0.11    0.00      0.07   0.97  1.16  ...   \n",
      "4  0.00     2.08  0.00  0.0  3.12  0.00    1.04      0.00   0.00  0.00  ...   \n",
      "\n",
      "   semicol  paren  bracket   bang  dollar  pound  cap_avg  cap_long  \\\n",
      "0    0.000  0.178      0.0  0.044   0.000   0.00    1.666        10   \n",
      "1    0.000  0.125      0.0  0.000   0.000   0.00    1.510        10   \n",
      "2    0.000  0.000      0.0  0.000   0.000   0.00    1.718        11   \n",
      "3    0.006  0.159      0.0  0.069   0.221   0.11    3.426        72   \n",
      "4    0.000  0.000      0.0  0.263   0.000   0.00    1.428         4   \n",
      "\n",
      "   cap_total  Class  \n",
      "0        180    ham  \n",
      "1         74    ham  \n",
      "2         55    ham  \n",
      "3        819   spam  \n",
      "4         20   spam  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "file_path = 'spam.csv'\n",
    "data = pd.read_csv(file_path, encoding='latin1')\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data['Class'] = label_encoder.fit_transform(data['Class'])\n",
    "\n",
    "X = data.drop(columns=['Class'])\n",
    "y = data['Class']\n",
    "\n",
    "X_train_initial, X_test_initial, y_train_initial, y_test_initial = X[:1000], X[1000:], y[:1000], y[1000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Split (1000/3601) Accuracy: 0.9230769230769231\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94      2182\n",
      "           1       0.90      0.91      0.90      1419\n",
      "\n",
      "    accuracy                           0.92      3601\n",
      "   macro avg       0.92      0.92      0.92      3601\n",
      "weighted avg       0.92      0.92      0.92      3601\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[2035  147]\n",
      " [ 130 1289]]\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "gnb_clf = GaussianNB()\n",
    "lr_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(max_iter=2000, solver='lbfgs', random_state=42))\n",
    "])\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('Decision Tree', dt_clf),\n",
    "        ('Naive Bayes', gnb_clf),\n",
    "        ('Logistic Regression', lr_clf)\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "voting_clf.fit(X_train_initial, y_train_initial)\n",
    "y_pred_initial = voting_clf.predict(X_test_initial)\n",
    "\n",
    "# Evaluation metrics\n",
    "print(\"Initial Split (1000/3601) Accuracy:\", accuracy_score(y_test_initial, y_pred_initial))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test_initial, y_pred_initial))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test_initial, y_pred_initial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 50-50%:\n",
      "Accuracy: 0.924380704041721\n",
      "Confusion Matrix:\n",
      " [[1279  122]\n",
      " [  52  848]]\n",
      "\n",
      "Classification Report:\n",
      "  Class 0:\n",
      "    Precision: 0.96\n",
      "    Recall: 0.91\n",
      "    F1-score: 0.94\n",
      "  Class 1:\n",
      "    Precision: 0.87\n",
      "    Recall: 0.94\n",
      "    F1-score: 0.91\n",
      "  Class macro avg:\n",
      "    Precision: 0.92\n",
      "    Recall: 0.93\n",
      "    F1-score: 0.92\n",
      "  Class weighted avg:\n",
      "    Precision: 0.93\n",
      "    Recall: 0.92\n",
      "    F1-score: 0.92\n",
      "\n",
      "Split 60-40%:\n",
      "Accuracy: 0.9282998370450842\n",
      "Confusion Matrix:\n",
      " [[1047   87]\n",
      " [  45  662]]\n",
      "\n",
      "Classification Report:\n",
      "  Class 0:\n",
      "    Precision: 0.96\n",
      "    Recall: 0.92\n",
      "    F1-score: 0.94\n",
      "  Class 1:\n",
      "    Precision: 0.88\n",
      "    Recall: 0.94\n",
      "    F1-score: 0.91\n",
      "  Class macro avg:\n",
      "    Precision: 0.92\n",
      "    Recall: 0.93\n",
      "    F1-score: 0.93\n",
      "  Class weighted avg:\n",
      "    Precision: 0.93\n",
      "    Recall: 0.93\n",
      "    F1-score: 0.93\n",
      "\n",
      "Split 70-30%:\n",
      "Accuracy: 0.9290369297610427\n",
      "Confusion Matrix:\n",
      " [[771  66]\n",
      " [ 32 512]]\n",
      "\n",
      "Classification Report:\n",
      "  Class 0:\n",
      "    Precision: 0.96\n",
      "    Recall: 0.92\n",
      "    F1-score: 0.94\n",
      "  Class 1:\n",
      "    Precision: 0.89\n",
      "    Recall: 0.94\n",
      "    F1-score: 0.91\n",
      "  Class macro avg:\n",
      "    Precision: 0.92\n",
      "    Recall: 0.93\n",
      "    F1-score: 0.93\n",
      "  Class weighted avg:\n",
      "    Precision: 0.93\n",
      "    Recall: 0.93\n",
      "    F1-score: 0.93\n",
      "\n",
      "Split 80-19%:\n",
      "Accuracy: 0.9196525515743756\n",
      "Confusion Matrix:\n",
      " [[503  52]\n",
      " [ 22 344]]\n",
      "\n",
      "Classification Report:\n",
      "  Class 0:\n",
      "    Precision: 0.96\n",
      "    Recall: 0.91\n",
      "    F1-score: 0.93\n",
      "  Class 1:\n",
      "    Precision: 0.87\n",
      "    Recall: 0.94\n",
      "    F1-score: 0.90\n",
      "  Class macro avg:\n",
      "    Precision: 0.91\n",
      "    Recall: 0.92\n",
      "    F1-score: 0.92\n",
      "  Class weighted avg:\n",
      "    Precision: 0.92\n",
      "    Recall: 0.92\n",
      "    F1-score: 0.92\n"
     ]
    }
   ],
   "source": [
    "splits = [0.5, 0.6, 0.7, 0.8]\n",
    "results = {}\n",
    "\n",
    "for split in splits:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1-split, random_state=42)\n",
    "\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "    results[split] = {\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Classification Report': classification_report(y_test, y_pred, output_dict=True),\n",
    "        'Confusion Matrix': confusion_matrix(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "for split, metrics in results.items():\n",
    "    print(f\"\\nSplit {int(split*100)}-{int((1-split)*100)}%:\")\n",
    "    print(\"Accuracy:\", metrics['Accuracy'])\n",
    "    print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    for label, report in metrics['Classification Report'].items():\n",
    "        if isinstance(report, dict):\n",
    "            print(f\"  Class {label}:\")\n",
    "            print(f\"    Precision: {report['precision']:.2f}\")\n",
    "            print(f\"    Recall: {report['recall']:.2f}\")\n",
    "            print(f\"    F1-score: {report['f1-score']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
